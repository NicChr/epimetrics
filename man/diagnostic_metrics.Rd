% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/diagnostic_metrics.R
\name{diagnostic_metrics}
\alias{diagnostic_metrics}
\alias{sensitivity}
\alias{tpr}
\alias{specificity}
\alias{tnr}
\alias{accuracy}
\alias{acc}
\alias{balanced_accuracy}
\alias{bac}
\alias{positive_predictive_value}
\alias{ppv}
\alias{negative_predictive_value}
\alias{npv}
\alias{positive_likelihood_ratio}
\alias{plr}
\alias{negative_likelihood_ratio}
\alias{nlr}
\alias{diagnostic_odds_ratio}
\alias{dor}
\alias{true_prevalence}
\alias{tpv}
\alias{apparent_prevalence}
\alias{apv}
\alias{prevalence_threshold}
\alias{prt}
\alias{youden_index}
\alias{yix}
\alias{number_needed_to_diagnose}
\alias{nnd}
\title{Diagnostic metrics to measure the performance of a clinical diagnostic test}
\usage{
diagnostic_metrics(cm, conf.int = TRUE, conf.level = 0.95, R = 10^5)

sensitivity(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

specificity(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

accuracy(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

balanced_accuracy(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

positive_predictive_value(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

negative_predictive_value(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

positive_likelihood_ratio(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

negative_likelihood_ratio(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

diagnostic_odds_ratio(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

true_prevalence(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

apparent_prevalence(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

prevalence_threshold(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95,
  R = 10^5
)

youden_index(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95
)

number_needed_to_diagnose(
  sensitivity,
  specificity,
  prevalence,
  x,
  conf.int = TRUE,
  conf.level = 0.95,
  R = 10^5
)
}
\arguments{
\item{cm}{A 2x2 confusion matrix containing data of
aggregate results and outcomes.
Accepted inputs are:
\itemize{
\item a vector or factor of the format \code{c(a,b,c,d)}
based on the table above.
\item a 2x2 table, matrix, or data.frame like the one above.
}}

\item{conf.int}{Logical. Should confidence intervals be added?}

\item{conf.level}{(1-alpha) Significance level for
confidence interval calculations.}

\item{R}{Number of simulation replicates. \cr}

\item{sensitivity}{Sensitivity or true positive rate of
diagnostic test.}

\item{specificity}{Specificity or true negative rate of
diagnostic test.}

\item{prevalence}{Prevalence or proportion of population
who have the disease,}

\item{x}{An n x 4 matrix containing the numbers of
(TP) true positives, (FP) false positives,
(FN) false negatives and (TN) true negatives
in order from left to right. \cr
This can easily be used with \code{simulate_confusion_data()},
see examples for more detail.\cr
You can supply anything that can be coerced to a numeric matrix
on a by-row basis.}
}
\value{
The function \code{diagnostic_metrics()}
returns a \code{data.frame} containing the variables:
\item{characteristic}{The diagnostic test metric.}
\item{abbreviation}{A 3-letter abbreviation of the characteristic.}
\item{estimate}{Point-estimate of the test characteristic.}
\item{conf.low}{Lower bound of the confidence interval.}
\item{conf.high}{Upper bound of the confidence interval.}

The other standalone metrics return either a
numeric vector estimate or matrix
containing an estimate and (optionally) confidence bounds. \cr
They can be called either by their lower-case 3-letter abbreviation,
or by the full name of the metric. \cr
If \code{x} is supplied then a \code{matrix} containing the
below 3 variables above is returned:

\item{estimate}{point-estimate of the data.}
\item{conf.low}{lower confidence bound of the estimate.}
\item{conf.high}{upper confidence bound of the estimate.}

If sensitivity, specificity and prevalence are supplied
then a numeric vector containing only the estimate is returned.

The following metrics which are all returned by \code{diagnostic_metrics()}
or produced individually using the standalone functions are described:
\item{TPR}{Sensitivity, or true positive rate.
This is the proportion of diseased individuals that test positive.
Pr(+Result|+Outcome)}
\item{TNR}{Specificity, or true negative rate.
This is the proportion of not diseased individuals that test negative.
Pr(-Result|-Outcome)}
\item{ACC}{Accuracy. Proportion of correctly classified individuals.}
\item{BAC}{Balanced accuracy. Median of Sensitivity and Specificity.}
\item{PPV}{Positive predictive value.
Proportion of individuals that have the disease
after receiving a positive result.
Pr(+Outcome|+Result)}
\item{NPV}{Negative predictive value.
Proportion of individuals that don't have the disease
after receiving a negative result.
Pr(-Outcome|-Result)}
\item{PLR}{Positive likelihood ratio.
Ratio of probability of true positive over false positive or
the odds of a positive result being correct.}
\item{NLR}{Negative likelihood ratio.
Ratio of probability of false negative over true negative or the
odds of negative result being incorrect.}
\item{DOR}{Diagnostic odds ratio.
Odds of testing positive if individual has the disease relative to the
odds of testing positive if the individual does not have the disease.}
\item{TPV}{True prevalence.
Proportion of individuals with the disease.}
\item{APV}{Apparent prevalence.
Proportion of individuals testing positive.}
\item{PRT}{Prevalence threshold.
Threshold defining the inflection point such that
the PPV of a diagnostic test drops
precipitously as prevalence decreases below this point.}
\item{YIX}{Youden index.
A diagnostic metric of performance which is the difference
between the rate of correctly classifying one group
against incorrectly classifying the other.
TPR - FPR = TNR - FNR = TPR + TNR - 1.}
\item{NND}{Number needed to diagnose. The number of patients
that need to be examined
in order to correctly detect one person with the disease of interest.}

All metrics/characteristics are accompanied with confidence intervals.
Wilson score intervals are used for most metrics,
except for the likelihood ratios and diagnostic odds ratio which use
an asymptotic normal approximation,
and the prevalence threshold which uses
Monte Carlo simulated percentile confidence intervals.
All wilson-score intervals are produced based on the
wilson-score method with exact boundary correction for extreme cases
except for the Youden index and number needed to diagnose metrics
which use a standard wilson-score (without continuity correction).

\bold{Notes on the Youden index}

Generally, any diagnostic test with a Youden index less than 0.5
is considered inadequate for diagnostic screening.
The Youden index can be used as a way of choosing a decision threshold in
binary classifier analyses,
where maximising the index across all
(sensitivity, 1-specificity) pairs on the ROC
curve results in the "optimal" cutoff.
This is mathematically equivalent to maximising
the vertical distance between the diagonal chance line and the curve.
Furthermore, cutoffs based on the Youden index implicitly assume that the
cost of a false positive is equal to the cost of a
false negative and hence may not be suitable in
real-world applications where type I errors may incur a
greater clinical, economic, or otherwise
cost relative to type II errors, or vice-versa.
}
\description{
Standard 2x2 confusion matrix:
\tabular{lcc}{
\tab  Disease \tab  No Disease  \cr
Positive \tab  a      \tab  b    \cr
Negative \tab  c      \tab  d   \cr
}

\bold{Important note} \cr
You can either supply \code{sensitivity}, \code{specificity} and
\code{prevalence} \bold{OR} \code{x}.
You cannot supply both as this will throw an error.
}
\examples{
library(epimetrics)
# Using the results from Anu Kantele et al. (2021),
# we can calculate the diagnostic accuracy of scent dogs 
# in detection of COVID-19
# in the randomised trial experiment compared 
# to a gold standard rt-PCR test.

# Using the results from the sniffed samples of all dogs, 
# we add the samples not sniffed and
# regard them as test-negatives
TP <- 392
FP <- 75
FN <- 35
TN <- 807
FN2 <- 456 - 427
TN2 <- 1224 - 882
confusion <- matrix(c(TP, FP, (FN + FN2),
                      (TN + TN2)), nrow = 2, ncol = 2, byrow = TRUE)
rownames(confusion) <- c("+", "-")
colnames(confusion) <- c("rt-PCR+", "rt-PCR-")
confusion
# All diagnostic metrics with CIs
diagnostic_metrics(confusion)

# You can use standalone metrics too
TPR <- sensitivity(x = confusion)
TNR <- specificity(x = confusion)
PPV <- ppv(x = confusion)
TPR
TNR
PPV

# power sample size calculation (based on normal-approximation)
# to achieve 90\% sensitivity and specificity
# Based on an expected 80\% probability that the lower bound of
# the 95\% CI will be greater than 80\%
p0 <- 0.8
p1 <- 0.9
alpha <- 0.05
power <- 0.8
beta <- 1 - power
z <- qnorm(1-(alpha/2))

n <- (p0*(1-p0)*(z + qnorm(1 - beta)*sqrt((p1*(1-p1)) / 
                                            ((p0*(1-p0)))))^2 ) / (p1 - p0)^2
cat("The minimal number of PCR+ and PCR- is:", ceiling(n))

# We can plot the changing predictive values as prevalence changes

prevalence <- seq(0, 1, 0.01)

plot(prevalence*100, ppv(TPR[,"estimate"],
                         TNR[,"estimate"],
                         prevalence) * 100,
     col = "blue",
     xlab = "Prevalence \%", ylab = "PPV \%",
     main = "Predictive value as prevalence changes")
points(prevalence * 100, npv(TPR[,"estimate"],
                             TNR[,"estimate"],
                             prevalence) * 100,
       col = "orange")
legend(60, 40,
       legend = c("PPV", "NPV", "Prevalence threshold"),
       col = c("blue", "orange", "black"),
       lty = c(2,2,3))
abline(v = prevalence_threshold(TPR[,"estimate"],
                                TNR[,"estimate"]) * 100,
       lty = 3)

n1 <- TP + FN + FN2 # rt-PCR+
n2 <- TN + TN2 + FP # rt-PCR-
n <- n1 + n2 # Number of samples

# Use our estimate of TPR and TNR to get expected numbers of outcomes
n1 <- round(n * prevalence)
n2 <- n - n1

TPR <- TPR[, "estimate"]
TNR <- TNR[, "estimate"]

TPs <- round(n1 * TPR)
FNs <- n1 - TPs
TNs <- round(n2 * TNR)
FPs <- n2 - TNs

x <- matrix(c(TPs, FPs, FNs, TNs), byrow = FALSE, ncol = 4)
sensitivities <- sensitivity(x = x)
sensitivities <- cbind(sensitivities, prevalence * 100)
# True Sensitivity doesn't change as it's not a function of prevalence
# Uncertainty in the data that produced our
# estimate increases as prevalences decreases
sensitivities

ppvs <- ppv(x = x)
ppvs <- cbind(ppvs, prevalence * 100)
ppvs
# We can see that in a low prevalence scenario (1\%), the ppv
# drops to as low as 13\% (95\%CI: 8\%-20\%)

# Use simulation to estimate uncertainty in any metric.
# Here we choose sensitivity and ppv
sim_cm <- simulate_confusion_data(TP, FP, (FN + FN2), (TN + TN2),
                                  R = 10^6)
TPRs <- sensitivity(x = sim_cm, conf.int = FALSE)[,"estimate"]
sim_sensitivities <- sensitivity(x = sim_cm)[,"estimate"]
hist(sim_sensitivities)
sim_sens_lcl <- quantile(sim_sensitivities, 0.05/2)
sim_sens_ucl <- quantile(sim_sensitivities, 1 - (0.05/2))
abline(v = sim_sens_lcl)
abline(v = sim_sens_ucl)
# Set conf.int = F for faster estimate
sim_ppvs <- as.numeric(ppv(x = sim_cm, conf.int = FALSE))
hist(sim_ppvs)
sim_ppv_lcl <- quantile(sim_ppvs, 0.05/2)
sim_ppv_ucl <- quantile(sim_ppvs, 1 - (0.05/2))
abline(v = sim_ppv_lcl)
abline(v = sim_ppv_ucl)

# Compare to the usual wilson-score method
c(sim_sens_lcl, sim_sens_ucl) # Percentile method
sensitivity(x = c(TP, FP, FN + FN2, TN + TN2)) # Wilson-score method
c(sim_ppv_lcl, sim_ppv_ucl) # Percentile method
ppv(x = c(TP, FP, FN + FN2, TN + TN2)) # Wilson-score method

}
\references{
Shan G. Improved Confidence Intervals for the Youden Index.
PLoS One. 2015 Jul 1;10(7):e0127272.
doi: \href{https://doi.org/10.1371/journal.pone.0127272}{10.1371/journal.pone.0127272}.
PMID: 26132806; PMCID: PMC4488538.

Bender R. Calculating confidence intervals for the number needed to treat.
Control Clin Trials. 2001 Apr;22(2):102-10.
doi: \href{https://doi.org/10.1016/s0197-2456(00)00134-3}{10.1016/s0197-2456(00)00134-3}.
PMID: 11306148.

Sakthivel Sivam, S. M.
"Everything or Nothing-A Better Confidence Intervals for Binomial
Proportion in Clinical Trial Data Analysis."
Sakthivel Sivam, Quartesian LLC,
Princeton, New Jersey Subbiah Meenakshisundaram, L. N Government College,
Ponneri, India 2016 (2014).
\url{https://www.lexjansen.com/pharmasug/2016/SP/PharmaSUG-2016-SP08.pdf}

Balayla J. Prevalence threshold and the geometry of screening curves.
PLoS One. 2020 Oct 7;15(10):e0240215.
doi: \href{https://doi.org/10.1371/journal.pone.0240215}{10.1371/journal.pone.0240215}.
PMID: 33027310; PMCID: PMC7540853.

Smits, N. A note on Youden's J and its cost ratio.
BMC Med Res Methodol 10, 89 (2010).
doi: \href{https://doi.org/10.1186/1471-2288-10-89}{10.1186/1471-2288-10-89}

Kantele A, Paajanen J, Turunen S, Pakkanen SH, Patjas A, Itkonen L,
Heiskanen E, Lappalainen M, Desquilbet L, Vapalahti O,
Hielm-BjÃ¶rkman A.
Scent dogs in detection of COVID-19: triple-blinded randomised trial
and operational real-life screening in airport setting.
BMJ Glob Health. 2022 May;7(5):e008024.
\href{https://doi.org/10.1136/bmjgh-2021-008024}{10.1136/bmjgh-2021-008024.}
PMID: 35577391; PMCID: PMC9108438.
}
